{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eaa8a009-3ae1-4dcd-b859-d8c2a1d6631e",
   "metadata": {},
   "source": [
    "\n",
    "Data profiling is critical for the success of the data loads.\n",
    "\n",
    "Profile across file is more important.\n",
    "\n",
    "In many scenarios, I have observed that the attribute behavior change between multiple files sent.\n",
    "\n",
    "There can be multiple reasons for that - \n",
    "e.g. \n",
    "1. Change in upstream environment DEV/UAT/PROD i.e. first file was generated from DEV env and later from different env and hence data difference may occur\n",
    "2. Change in business logic\n",
    "3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd3b975d-2cf1-4541-a834-557ba65cac3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f947f753-4b9a-4aff-93e7-0838f53e4313",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\").appName('PySpark_Tutorial').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c647f4e8-24f7-47e5-a104-ce5b10387a33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LAPTOP-58RVEH10:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySpark_Tutorial</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1c230e50988>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05fe37d6-3e34-4a6b-a065-02e143acb06d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Platform =  Windows-10-10.0.19041-SP0\n",
      "Version of Spark =  3.2.0\n",
      "Python version =  3.7.11 (default, Jul 27 2021, 09:42:29) [MSC v.1916 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import platform, sys, os\n",
    "print('Platform = ',platform.platform())  \n",
    "print('Version of Spark = ',spark.version)\n",
    "print('Python version = ',sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24aceab1-2ca5-4ca9-bd1b-67452706c00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "\n",
    "def dataprofile(data_all_df,data_cols):\n",
    "    data_df = data_all_df.select(data_cols)\n",
    "    columns2Bprofiled = data_df.columns\n",
    "    global schema_name, table_name\n",
    "    if not 'schema_name' in globals():\n",
    "        schema_name = 'schema_name'\n",
    "    if not 'table_name' in globals():\n",
    "        table_name = 'table_name' \n",
    "    dprof_df = pd.DataFrame({'schema_name':[schema_name] * len(data_df.columns),\\\n",
    "                             'table_name':[table_name] * len(data_df.columns),\\\n",
    "                             'column_names':data_df.columns,\\\n",
    "                             'data_types':[x[1] for x in data_df.dtypes]}) \n",
    "    dprof_df = dprof_df[['schema_name','table_name','column_names', 'data_types']]\n",
    "    dprof_df.set_index('column_names', inplace=True, drop=False)\n",
    "    # ======================\n",
    "    num_rows = data_df.count()\n",
    "    dprof_df['num_rows'] = num_rows\n",
    "    # ======================    \n",
    "    # number of rows with nulls and nans   \n",
    "    df_nacounts = data_df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in data_df.columns \\\n",
    "                                  if data_df.select(c).dtypes[0][1]!='timestamp']).toPandas().transpose()\n",
    "    df_nacounts = df_nacounts.reset_index()  \n",
    "    df_nacounts.columns = ['column_names','num_null']\n",
    "    dprof_df = pd.merge(dprof_df, df_nacounts, on = ['column_names'], how = 'left')\n",
    "    # ========================\n",
    "    # number of rows with white spaces (one or more space) or blanks\n",
    "    num_spaces = [data_df.where(F.col(c).rlike('^\\\\s+$')).count() for c in data_df.columns]\n",
    "    dprof_df['num_spaces'] = num_spaces\n",
    "    num_blank = [data_df.where(F.col(c)=='').count() for c in data_df.columns]\n",
    "    dprof_df['num_blank'] = num_blank\n",
    "    # =========================\n",
    "    # using the in built describe() function \n",
    "    desc_df = data_df.describe().toPandas().transpose()\n",
    "    desc_df.columns = ['count', 'mean', 'stddev', 'min', 'max']\n",
    "    desc_df = desc_df.iloc[1:,:]  \n",
    "    desc_df = desc_df.reset_index()  \n",
    "    desc_df.columns.values[0] = 'column_names'  \n",
    "    desc_df = desc_df[['column_names','count', 'mean', 'stddev']] \n",
    "    dprof_df = pd.merge(dprof_df, desc_df , on = ['column_names'], how = 'left')\n",
    "    # ===========================================\n",
    "    allminvalues = [data_df.select(F.min(x)).limit(1).toPandas().iloc[0][0] for x in columns2Bprofiled]\n",
    "    allmaxvalues = [data_df.select(F.max(x)).limit(1).toPandas().iloc[0][0] for x in columns2Bprofiled]\n",
    "    allmincounts = [data_df.where(col(x) == y).count() for x,y in zip(columns2Bprofiled, allminvalues)]\n",
    "    allmaxcounts = [data_df.where(col(x) == y).count() for x,y in zip(columns2Bprofiled, allmaxvalues)]    \n",
    "    df_counts = dprof_df[['column_names']]\n",
    "    df_counts.insert(loc=0, column='min', value=allminvalues)\n",
    "    df_counts.insert(loc=0, column='counts_min', value=allmincounts)\n",
    "    df_counts.insert(loc=0, column='max', value=allmaxvalues)\n",
    "    df_counts.insert(loc=0, column='counts_max', value=allmaxcounts)\n",
    "    df_counts = df_counts[['column_names','min','counts_min','max','counts_max']]\n",
    "    dprof_df = pd.merge(dprof_df, df_counts , on = ['column_names'], how = 'left')  \n",
    "    # ==========================================\n",
    "    # number of distinct values in each column\n",
    "    dprof_df['num_distinct'] = [data_df.select(x).distinct().count() for x in columns2Bprofiled]\n",
    "    # ============================================\n",
    "    # most frequently occuring value in a column and its count\n",
    "    dprof_df['most_freq_valwcount'] = [data_df.groupBy(x).count().sort(\"count\",ascending=False).limit(1).\\\n",
    "                                       toPandas().iloc[0].values.tolist() for x in columns2Bprofiled]\n",
    "    dprof_df['most_freq_value'] = [x[0] for x in dprof_df['most_freq_valwcount']]\n",
    "    dprof_df['most_freq_value_count'] = [x[1] for x in dprof_df['most_freq_valwcount']]\n",
    "    dprof_df = dprof_df.drop(['most_freq_valwcount'],axis=1)\n",
    "    # least frequently occuring value in a column and its count\n",
    "    dprof_df['least_freq_valwcount'] = [data_df.groupBy(x).count().sort(\"count\",ascending=True).limit(1).\\\n",
    "                                        toPandas().iloc[0].values.tolist() for x in columns2Bprofiled]\n",
    "    dprof_df['least_freq_value'] = [x[0] for x in dprof_df['least_freq_valwcount']]\n",
    "    dprof_df['least_freq_value_count'] = [x[1] for x in dprof_df['least_freq_valwcount']]\n",
    "    dprof_df = dprof_df.drop(['least_freq_valwcount'],axis=1)\n",
    "\n",
    "    return dprof_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "313a2d85-fc27-49f9-a661-d6be2a7065b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28668516-3393-4b30-98a7-1da2db302a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.text(r'file:/D:/Projects/saket1471/learn-spark/data/netflix_titles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a7ee785-e1b9-49d8-96d5-8471fce22cfc",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|show_id,type,titl...|\n",
      "|s1,Movie,Dick Joh...|\n",
      "|s2,TV Show,Blood ...|\n",
      "|s3,TV Show,Gangla...|\n",
      "|s4,TV Show,Jailbi...|\n",
      "|s5,TV Show,Kota F...|\n",
      "|s6,TV Show,Midnig...|\n",
      "|s7,Movie,My Littl...|\n",
      "|s8,Movie,Sankofa,...|\n",
      "|s9,TV Show,The Gr...|\n",
      "|s10,Movie,The Sta...|\n",
      "|s11,TV Show,\"Vend...|\n",
      "|s12,TV Show,Bangk...|\n",
      "|s13,Movie,Je Suis...|\n",
      "|s14,Movie,Confess...|\n",
      "|s15,TV Show,Crime...|\n",
      "|s16,TV Show,Dear ...|\n",
      "|s17,Movie,Europe'...|\n",
      "|s18,TV Show,Falsa...|\n",
      "|s19,Movie,Intrusi...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "afa0d947-0f46-4068-9674-a95ad12c87a4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "text() got an unexpected keyword argument 'sep'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21628/2425223968.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'file:/D:/Projects/saket1471/learn-spark/data/netflix_titles.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\",\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: text() got an unexpected keyword argument 'sep'"
     ]
    }
   ],
   "source": [
    "df = spark.read.text(r'file:/D:/Projects/saket1471/learn-spark/data/netflix_titles.csv', sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e6b7742-3c70-4de3-82a5-fbc90c742479",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method text in module pyspark.sql.readwriter:\n",
      "\n",
      "text(paths, wholetext=False, lineSep=None, pathGlobFilter=None, recursiveFileLookup=None, modifiedBefore=None, modifiedAfter=None) method of pyspark.sql.readwriter.DataFrameReader instance\n",
      "    Loads text files and returns a :class:`DataFrame` whose schema starts with a\n",
      "    string column named \"value\", and followed by partitioned columns if there\n",
      "    are any.\n",
      "    The text files must be encoded as UTF-8.\n",
      "    \n",
      "    By default, each line in the text file is a new row in the resulting DataFrame.\n",
      "    \n",
      "    .. versionadded:: 1.6.0\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    paths : str or list\n",
      "        string, or list of strings, for input path(s).\n",
      "    \n",
      "    Other Parameters\n",
      "    ----------------\n",
      "    Extra options\n",
      "        For the extra options, refer to\n",
      "        `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-text.html#data-source-option>`_\n",
      "        in the version you use.\n",
      "    \n",
      "        .. # noqa\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df = spark.read.text('python/test_support/sql/text-test.txt')\n",
      "    >>> df.collect()\n",
      "    [Row(value='hello'), Row(value='this')]\n",
      "    >>> df = spark.read.text('python/test_support/sql/text-test.txt', wholetext=True)\n",
      "    >>> df.collect()\n",
      "    [Row(value='hello\\nthis')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(spark.read.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c65b8bf-dd77-4c57-a420-b2e0b11602d6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'option'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21628/1536435383.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'file:/D:/Projects/saket1471/learn-spark/data/netflix_titles.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moption\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\",\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\.conda\\envs\\pff\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1658\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1659\u001b[0m             raise AttributeError(\n\u001b[1;32m-> 1660\u001b[1;33m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001b[0m\u001b[0;32m   1661\u001b[0m         \u001b[0mjc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1662\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'option'"
     ]
    }
   ],
   "source": [
    "df = spark.read.text(r'file:/D:/Projects/saket1471/learn-spark/data/netflix_titles.csv').option(sep,\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "30a5c6f3-78ad-43ca-8e6d-3ee4aec01fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.options(delimiter=\",\", header=True).text(r'file:/D:/Projects/saket1471/learn-spark/data/netflix_titles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2fcf774a-6a07-4ba2-97e3-40b6b3752de4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|show_id,type,titl...|\n",
      "|s1,Movie,Dick Joh...|\n",
      "|s2,TV Show,Blood ...|\n",
      "|s3,TV Show,Gangla...|\n",
      "|s4,TV Show,Jailbi...|\n",
      "|s5,TV Show,Kota F...|\n",
      "|s6,TV Show,Midnig...|\n",
      "|s7,Movie,My Littl...|\n",
      "|s8,Movie,Sankofa,...|\n",
      "|s9,TV Show,The Gr...|\n",
      "|s10,Movie,The Sta...|\n",
      "|s11,TV Show,\"Vend...|\n",
      "|s12,TV Show,Bangk...|\n",
      "|s13,Movie,Je Suis...|\n",
      "|s14,Movie,Confess...|\n",
      "|s15,TV Show,Crime...|\n",
      "|s16,TV Show,Dear ...|\n",
      "|s17,Movie,Europe'...|\n",
      "|s18,TV Show,Falsa...|\n",
      "|s19,Movie,Intrusi...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa53758a-9e76-4c4f-9a15-5e2a7204e508",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.options(delimiter=\",\", header=True).csv(r'file:/D:/Projects/saket1471/learn-spark/data/netflix_titles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f9ba9c03-3517-4301-b1cd-2b58004d3a77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['show_id',\n",
       " 'type',\n",
       " 'title',\n",
       " 'director',\n",
       " 'cast',\n",
       " 'country',\n",
       " 'date_added',\n",
       " 'release_year',\n",
       " 'rating',\n",
       " 'duration',\n",
       " 'listed_in',\n",
       " 'description']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "66f9321d-c8be-40b4-85cd-10efe5911e0e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------------------+--------------------+--------------------+--------------------+------------------+------------+------+---------+--------------------+--------------------+\n",
      "|show_id|   type|               title|            director|                cast|             country|        date_added|release_year|rating| duration|           listed_in|         description|\n",
      "+-------+-------+--------------------+--------------------+--------------------+--------------------+------------------+------------+------+---------+--------------------+--------------------+\n",
      "|     s1|  Movie|Dick Johnson Is Dead|     Kirsten Johnson|                null|       United States|September 25, 2021|        2020| PG-13|   90 min|       Documentaries|As her father nea...|\n",
      "|     s2|TV Show|       Blood & Water|                null|Ama Qamata, Khosi...|        South Africa|September 24, 2021|        2021| TV-MA|2 Seasons|International TV ...|After crossing pa...|\n",
      "|     s3|TV Show|           Ganglands|     Julien Leclercq|Sami Bouajila, Tr...|                null|September 24, 2021|        2021| TV-MA| 1 Season|Crime TV Shows, I...|To protect his fa...|\n",
      "|     s4|TV Show|Jailbirds New Orl...|                null|                null|                null|September 24, 2021|        2021| TV-MA| 1 Season|Docuseries, Reali...|Feuds, flirtation...|\n",
      "|     s5|TV Show|        Kota Factory|                null|Mayur More, Jiten...|               India|September 24, 2021|        2021| TV-MA|2 Seasons|International TV ...|In a city of coac...|\n",
      "|     s6|TV Show|       Midnight Mass|       Mike Flanagan|Kate Siegel, Zach...|                null|September 24, 2021|        2021| TV-MA| 1 Season|TV Dramas, TV Hor...|The arrival of a ...|\n",
      "|     s7|  Movie|My Little Pony: A...|Robert Cullen, Jo...|Vanessa Hudgens, ...|                null|September 24, 2021|        2021|    PG|   91 min|Children & Family...|Equestria's divid...|\n",
      "|     s8|  Movie|             Sankofa|        Haile Gerima|Kofi Ghanaba, Oya...|United States, Gh...|September 24, 2021|        1993| TV-MA|  125 min|Dramas, Independe...|On a photo shoot ...|\n",
      "|     s9|TV Show|The Great British...|     Andy Devonshire|Mel Giedroyc, Sue...|      United Kingdom|September 24, 2021|        2021| TV-14|9 Seasons|British TV Shows,...|A talented batch ...|\n",
      "|    s10|  Movie|        The Starling|      Theodore Melfi|Melissa McCarthy,...|       United States|September 24, 2021|        2021| PG-13|  104 min|    Comedies, Dramas|A woman adjusting...|\n",
      "|    s11|TV Show|Vendetta: Truth, ...|                null|                null|                null|September 24, 2021|        2021| TV-MA| 1 Season|Crime TV Shows, D...|\"Sicily boasts a ...|\n",
      "|    s12|TV Show|    Bangkok Breaking|   Kongkiat Komesiri|Sukollawat Kanaro...|                null|September 23, 2021|        2021| TV-MA| 1 Season|Crime TV Shows, I...|Struggling to ear...|\n",
      "|    s13|  Movie|        Je Suis Karl| Christian Schwochow|Luna Wedler, Jann...|Germany, Czech Re...|September 23, 2021|        2021| TV-MA|  127 min|Dramas, Internati...|After most of her...|\n",
      "|    s14|  Movie|Confessions of an...|       Bruno Garotti|Klara Castanho, L...|                null|September 22, 2021|        2021| TV-PG|   91 min|Children & Family...|When the clever b...|\n",
      "|    s15|TV Show|Crime Stories: In...|                null|                null|                null|September 22, 2021|        2021| TV-MA| 1 Season|British TV Shows,...|Cameras following...|\n",
      "|    s16|TV Show|   Dear White People|                null|Logan Browning, B...|       United States|September 22, 2021|        2021| TV-MA|4 Seasons|TV Comedies, TV D...|\"Students of colo...|\n",
      "|    s17|  Movie|Europe's Most Dan...|Pedro de Echave G...|                null|                null|September 22, 2021|        2020| TV-MA|   67 min|Documentaries, In...|Declassified docu...|\n",
      "|    s18|TV Show|     Falsa identidad|                null|Luis Ernesto Fran...|              Mexico|September 22, 2021|        2020| TV-MA|2 Seasons|Crime TV Shows, S...|Strangers Diego a...|\n",
      "|    s19|  Movie|           Intrusion|          Adam Salky|Freida Pinto, Log...|                null|September 22, 2021|        2021| TV-14|   94 min|           Thrillers|After a deadly ho...|\n",
      "|    s20|TV Show|              Jaguar|                null|Blanca Suárez, Iv...|                null|September 22, 2021|        2021| TV-MA| 1 Season|International TV ...|In the 1960s, a H...|\n",
      "+-------+-------+--------------------+--------------------+--------------------+--------------------+------------------+------------+------+---------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "468503d2-c75a-4ff5-973e-b91116379301",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'column_names' is both an index level and a column label, which is ambiguous.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21628/3489367151.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mcols2profile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m  \u001b[1;31m# select all or some columns from the table\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mdprofile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataprofile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcols2profile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Time taken to execute dataprofile function '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mend\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m' minutes'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21628/2937741331.py\u001b[0m in \u001b[0;36mdataprofile\u001b[1;34m(data_all_df, data_cols)\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mdf_nacounts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_nacounts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0mdf_nacounts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'column_names'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'num_null'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[0mdprof_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdprof_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_nacounts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'column_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'left'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m     \u001b[1;31m# ========================\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;31m# number of rows with white spaces (one or more space) or blanks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\pff\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[0mindicator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindicator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m         \u001b[0mvalidate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m     )\n\u001b[0;32m    121\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\pff\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    697\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mright_join_keys\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    698\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 699\u001b[1;33m         ) = self._get_merge_keys()\n\u001b[0m\u001b[0;32m    700\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m         \u001b[1;31m# validate the merge keys dtypes. We may need to coerce\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\pff\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m_get_merge_keys\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1107\u001b[0m                         \u001b[0mright_keys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1108\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mlk\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1109\u001b[1;33m                         \u001b[0mleft_keys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1110\u001b[0m                         \u001b[0mjoin_names\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1111\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\pff\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_get_label_or_level_values\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1772\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1773\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_label_reference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1774\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_label_or_level_ambiguity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1775\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mother_axes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1776\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_level_reference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\pff\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_check_label_or_level_ambiguity\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1731\u001b[0m                 \u001b[1;34mf\"{label_article} {label_type} label, which is ambiguous.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1732\u001b[0m             )\n\u001b[1;32m-> 1733\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1734\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1735\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: 'column_names' is both an index level and a column label, which is ambiguous."
     ]
    }
   ],
   "source": [
    "# Driver code for the data profle function\n",
    "import time\n",
    "start = time.time()\n",
    "cols2profile = df.columns  # select all or some columns from the table\n",
    "dprofile = dataprofile(df, cols2profile)\n",
    "end = time.time()\n",
    "print('Time taken to execute dataprofile function ', (end - start)/60,' minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f529f13a-c4f7-47ea-a077-fc565d01067a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
