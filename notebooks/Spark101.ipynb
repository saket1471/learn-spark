{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4c63866-57dc-49be-8a61-70a20a6b9aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a428c35d-daf4-46c0-afc1-fe1762dea21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\").appName('PySpark_Tutorial').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8392e49-10ba-431b-8df8-caf599b529da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LAPTOP-58RVEH10:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySpark_Tutorial</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x299ea4707c8>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df5f4338-3561-41d6-b302-d26d85bb07aa",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Builder',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__enter__',\n",
       " '__eq__',\n",
       " '__exit__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_activeSession',\n",
       " '_conf',\n",
       " '_convert_from_pandas',\n",
       " '_createFromLocal',\n",
       " '_createFromRDD',\n",
       " '_create_dataframe',\n",
       " '_create_from_pandas_with_arrow',\n",
       " '_create_shell_session',\n",
       " '_get_numpy_record_dtype',\n",
       " '_inferSchema',\n",
       " '_inferSchemaFromList',\n",
       " '_instantiatedSession',\n",
       " '_jsc',\n",
       " '_jsparkSession',\n",
       " '_jvm',\n",
       " '_jwrapped',\n",
       " '_repr_html_',\n",
       " '_sc',\n",
       " '_wrapped',\n",
       " 'builder',\n",
       " 'catalog',\n",
       " 'conf',\n",
       " 'createDataFrame',\n",
       " 'getActiveSession',\n",
       " 'newSession',\n",
       " 'range',\n",
       " 'read',\n",
       " 'readStream',\n",
       " 'sparkContext',\n",
       " 'sql',\n",
       " 'stop',\n",
       " 'streams',\n",
       " 'table',\n",
       " 'udf',\n",
       " 'version']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d827f9a-c3db-4c8a-8bb8-d4c5d5eafa59",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on SparkSession in module pyspark.sql.session object:\n",
      "\n",
      "class SparkSession(pyspark.sql.pandas.conversion.SparkConversionMixin)\n",
      " |  SparkSession(sparkContext, jsparkSession=None)\n",
      " |  \n",
      " |  The entry point to programming Spark with the Dataset and DataFrame API.\n",
      " |  \n",
      " |  A SparkSession can be used create :class:`DataFrame`, register :class:`DataFrame` as\n",
      " |  tables, execute SQL over tables, cache tables, and read parquet files.\n",
      " |  To create a :class:`SparkSession`, use the following builder pattern:\n",
      " |  \n",
      " |  .. autoattribute:: builder\n",
      " |     :annotation:\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> spark = SparkSession.builder \\\n",
      " |  ...     .master(\"local\") \\\n",
      " |  ...     .appName(\"Word Count\") \\\n",
      " |  ...     .config(\"spark.some.config.option\", \"some-value\") \\\n",
      " |  ...     .getOrCreate()\n",
      " |  \n",
      " |  >>> from datetime import datetime\n",
      " |  >>> from pyspark.sql import Row\n",
      " |  >>> spark = SparkSession(sc)\n",
      " |  >>> allTypes = sc.parallelize([Row(i=1, s=\"string\", d=1.0, l=1,\n",
      " |  ...     b=True, list=[1, 2, 3], dict={\"s\": 0}, row=Row(a=1),\n",
      " |  ...     time=datetime(2014, 8, 1, 14, 1, 5))])\n",
      " |  >>> df = allTypes.toDF()\n",
      " |  >>> df.createOrReplaceTempView(\"allTypes\")\n",
      " |  >>> spark.sql('select i+1, d+1, not b, list[1], dict[\"s\"], time, row.a '\n",
      " |  ...            'from allTypes where b and i > 0').collect()\n",
      " |  [Row((i + 1)=2, (d + 1)=2.0, (NOT b)=False, list[1]=2,         dict[s]=0, time=datetime.datetime(2014, 8, 1, 14, 1, 5), a=1)]\n",
      " |  >>> df.rdd.map(lambda x: (x.i, x.s, x.d, x.l, x.b, x.time, x.row.a, x.list)).collect()\n",
      " |  [(1, 'string', 1.0, 1, True, datetime.datetime(2014, 8, 1, 14, 1, 5), 1, [1, 2, 3])]\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      SparkSession\n",
      " |      pyspark.sql.pandas.conversion.SparkConversionMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __enter__(self)\n",
      " |      Enable 'with SparkSession.builder.(...).getOrCreate() as session: app' syntax.\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  __exit__(self, exc_type, exc_val, exc_tb)\n",
      " |      Enable 'with SparkSession.builder.(...).getOrCreate() as session: app' syntax.\n",
      " |      \n",
      " |      Specifically stop the SparkSession on exit of the with block.\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  __init__(self, sparkContext, jsparkSession=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  createDataFrame(self, data, schema=None, samplingRatio=None, verifySchema=True)\n",
      " |      Creates a :class:`DataFrame` from an :class:`RDD`, a list or a :class:`pandas.DataFrame`.\n",
      " |      \n",
      " |      When ``schema`` is a list of column names, the type of each column\n",
      " |      will be inferred from ``data``.\n",
      " |      \n",
      " |      When ``schema`` is ``None``, it will try to infer the schema (column names and types)\n",
      " |      from ``data``, which should be an RDD of either :class:`Row`,\n",
      " |      :class:`namedtuple`, or :class:`dict`.\n",
      " |      \n",
      " |      When ``schema`` is :class:`pyspark.sql.types.DataType` or a datatype string, it must match\n",
      " |      the real data, or an exception will be thrown at runtime. If the given schema is not\n",
      " |      :class:`pyspark.sql.types.StructType`, it will be wrapped into a\n",
      " |      :class:`pyspark.sql.types.StructType` as its only field, and the field name will be \"value\".\n",
      " |      Each record will also be wrapped into a tuple, which can be converted to row later.\n",
      " |      \n",
      " |      If schema inference is needed, ``samplingRatio`` is used to determined the ratio of\n",
      " |      rows used for schema inference. The first row will be used if ``samplingRatio`` is ``None``.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |      \n",
      " |      .. versionchanged:: 2.1.0\n",
      " |         Added verifySchema.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data : :class:`RDD` or iterable\n",
      " |          an RDD of any kind of SQL data representation (:class:`Row`,\n",
      " |          :class:`tuple`, ``int``, ``boolean``, etc.), or :class:`list`, or\n",
      " |          :class:`pandas.DataFrame`.\n",
      " |      schema : :class:`pyspark.sql.types.DataType`, str or list, optional\n",
      " |          a :class:`pyspark.sql.types.DataType` or a datatype string or a list of\n",
      " |          column names, default is None.  The data type string format equals to\n",
      " |          :class:`pyspark.sql.types.DataType.simpleString`, except that top level struct type can\n",
      " |          omit the ``struct<>`` and atomic types use ``typeName()`` as their format, e.g. use\n",
      " |          ``byte`` instead of ``tinyint`` for :class:`pyspark.sql.types.ByteType`.\n",
      " |          We can also use ``int`` as a short name for :class:`pyspark.sql.types.IntegerType`.\n",
      " |      samplingRatio : float, optional\n",
      " |          the sample ratio of rows used for inferring\n",
      " |      verifySchema : bool, optional\n",
      " |          verify data types of every row against schema. Enabled by default.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Usage with spark.sql.execution.arrow.pyspark.enabled=True is experimental.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> l = [('Alice', 1)]\n",
      " |      >>> spark.createDataFrame(l).collect()\n",
      " |      [Row(_1='Alice', _2=1)]\n",
      " |      >>> spark.createDataFrame(l, ['name', 'age']).collect()\n",
      " |      [Row(name='Alice', age=1)]\n",
      " |      \n",
      " |      >>> d = [{'name': 'Alice', 'age': 1}]\n",
      " |      >>> spark.createDataFrame(d).collect()\n",
      " |      [Row(age=1, name='Alice')]\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize(l)\n",
      " |      >>> spark.createDataFrame(rdd).collect()\n",
      " |      [Row(_1='Alice', _2=1)]\n",
      " |      >>> df = spark.createDataFrame(rdd, ['name', 'age'])\n",
      " |      >>> df.collect()\n",
      " |      [Row(name='Alice', age=1)]\n",
      " |      \n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> Person = Row('name', 'age')\n",
      " |      >>> person = rdd.map(lambda r: Person(*r))\n",
      " |      >>> df2 = spark.createDataFrame(person)\n",
      " |      >>> df2.collect()\n",
      " |      [Row(name='Alice', age=1)]\n",
      " |      \n",
      " |      >>> from pyspark.sql.types import *\n",
      " |      >>> schema = StructType([\n",
      " |      ...    StructField(\"name\", StringType(), True),\n",
      " |      ...    StructField(\"age\", IntegerType(), True)])\n",
      " |      >>> df3 = spark.createDataFrame(rdd, schema)\n",
      " |      >>> df3.collect()\n",
      " |      [Row(name='Alice', age=1)]\n",
      " |      \n",
      " |      >>> spark.createDataFrame(df.toPandas()).collect()  # doctest: +SKIP\n",
      " |      [Row(name='Alice', age=1)]\n",
      " |      >>> spark.createDataFrame(pandas.DataFrame([[1, 2]])).collect()  # doctest: +SKIP\n",
      " |      [Row(0=1, 1=2)]\n",
      " |      \n",
      " |      >>> spark.createDataFrame(rdd, \"a: string, b: int\").collect()\n",
      " |      [Row(a='Alice', b=1)]\n",
      " |      >>> rdd = rdd.map(lambda row: row[1])\n",
      " |      >>> spark.createDataFrame(rdd, \"int\").collect()\n",
      " |      [Row(value=1)]\n",
      " |      >>> spark.createDataFrame(rdd, \"boolean\").collect() # doctest: +IGNORE_EXCEPTION_DETAIL\n",
      " |      Traceback (most recent call last):\n",
      " |          ...\n",
      " |      Py4JJavaError: ...\n",
      " |  \n",
      " |  newSession(self)\n",
      " |      Returns a new :class:`SparkSession` as new session, that has separate SQLConf,\n",
      " |      registered temporary views and UDFs, but shared :class:`SparkContext` and\n",
      " |      table cache.\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  range(self, start, end=None, step=1, numPartitions=None)\n",
      " |      Create a :class:`DataFrame` with single :class:`pyspark.sql.types.LongType` column named\n",
      " |      ``id``, containing elements in a range from ``start`` to ``end`` (exclusive) with\n",
      " |      step value ``step``.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      start : int\n",
      " |          the start value\n",
      " |      end : int, optional\n",
      " |          the end value (exclusive)\n",
      " |      step : int, optional\n",
      " |          the incremental step (default: 1)\n",
      " |      numPartitions : int, optional\n",
      " |          the number of partitions of the DataFrame\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> spark.range(1, 7, 2).collect()\n",
      " |      [Row(id=1), Row(id=3), Row(id=5)]\n",
      " |      \n",
      " |      If only one argument is specified, it will be used as the end value.\n",
      " |      \n",
      " |      >>> spark.range(3).collect()\n",
      " |      [Row(id=0), Row(id=1), Row(id=2)]\n",
      " |  \n",
      " |  sql(self, sqlQuery)\n",
      " |      Returns a :class:`DataFrame` representing the result of the given query.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.createOrReplaceTempView(\"table1\")\n",
      " |      >>> df2 = spark.sql(\"SELECT field1 AS f1, field2 as f2 from table1\")\n",
      " |      >>> df2.collect()\n",
      " |      [Row(f1=1, f2='row1'), Row(f1=2, f2='row2'), Row(f1=3, f2='row3')]\n",
      " |  \n",
      " |  stop(self)\n",
      " |      Stop the underlying :class:`SparkContext`.\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  table(self, tableName)\n",
      " |      Returns the specified table as a :class:`DataFrame`.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.createOrReplaceTempView(\"table1\")\n",
      " |      >>> df2 = spark.table(\"table1\")\n",
      " |      >>> sorted(df.collect()) == sorted(df2.collect())\n",
      " |      True\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  getActiveSession() from builtins.type\n",
      " |      Returns the active :class:`SparkSession` for the current thread, returned by the builder\n",
      " |      \n",
      " |      .. versionadded:: 3.0.0\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`SparkSession`\n",
      " |          Spark session if an active session exists for the current thread\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> s = SparkSession.getActiveSession()\n",
      " |      >>> l = [('Alice', 1)]\n",
      " |      >>> rdd = s.sparkContext.parallelize(l)\n",
      " |      >>> df = s.createDataFrame(rdd, ['name', 'age'])\n",
      " |      >>> df.select(\"age\").collect()\n",
      " |      [Row(age=1)]\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  catalog\n",
      " |      Interface through which the user may create, drop, alter or query underlying\n",
      " |      databases, tables, functions, etc.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`Catalog`\n",
      " |  \n",
      " |  conf\n",
      " |      Runtime configuration interface for Spark.\n",
      " |      \n",
      " |      This is the interface through which the user can get and set all Spark and Hadoop\n",
      " |      configurations that are relevant to Spark SQL. When getting the value of a config,\n",
      " |      this defaults to the value set in the underlying :class:`SparkContext`, if any.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`pyspark.sql.conf.RuntimeConfig`\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  read\n",
      " |      Returns a :class:`DataFrameReader` that can be used to read data\n",
      " |      in as a :class:`DataFrame`.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrameReader`\n",
      " |  \n",
      " |  readStream\n",
      " |      Returns a :class:`DataStreamReader` that can be used to read data streams\n",
      " |      as a streaming :class:`DataFrame`.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This API is evolving.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataStreamReader`\n",
      " |  \n",
      " |  sparkContext\n",
      " |      Returns the underlying :class:`SparkContext`.\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  streams\n",
      " |      Returns a :class:`StreamingQueryManager` that allows managing all the\n",
      " |      :class:`StreamingQuery` instances active on `this` context.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This API is evolving.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`StreamingQueryManager`\n",
      " |  \n",
      " |  udf\n",
      " |      Returns a :class:`UDFRegistration` for UDF registration.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`UDFRegistration`\n",
      " |  \n",
      " |  version\n",
      " |      The version of Spark on which this application is running.\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  Builder = <class 'pyspark.sql.session.SparkSession.Builder'>\n",
      " |      Builder for :class:`SparkSession`.\n",
      " |  \n",
      " |  builder = <pyspark.sql.session.SparkSession.Builder object>\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pyspark.sql.pandas.conversion.SparkConversionMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72de5c2f-0e08-4351-a4c4-24bddf281a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading CSV file\n",
    "\n",
    "csv_file = 'data/stocks_price_final.csv'\n",
    "df = spark.read.csv(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1caf94e-0cfc-406b-b0d3-8274942bb223",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Reading JSON file\n",
    "\n",
    "# json_file = 'data/stocks_price_final.json'\n",
    "# data = spark.read.json(json_file)\n",
    "\n",
    "# # Reading parquet file\n",
    "\n",
    "# parquet_file = 'data/stocks_price_final.parquet'\n",
    "# data1 = spark.read.parquet(parquet_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52b709e9-ec1c-4a81-875b-be1a313a66b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: string, _c1: string, _c2: string, _c3: string, _c4: string, _c5: string, _c6: string, _c7: string, _c8: string, _c9: string, _c10: string, _c11: string, _c12: string]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83c2d294-b172-4ab9-ba44-53cd953d9b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+----------+---------+---------+---------+---------+-------+---------+----------+-------------+--------------------+--------+\n",
      "| _c0|   _c1|       _c2|      _c3|      _c4|      _c5|      _c6|    _c7|      _c8|       _c9|         _c10|                _c11|    _c12|\n",
      "+----+------+----------+---------+---------+---------+---------+-------+---------+----------+-------------+--------------------+--------+\n",
      "|null|symbol|      date|     open|     high|      low|    close| volume| adjusted|market.cap|       sector|            industry|exchange|\n",
      "|   1|   TXG|2019-09-12|       54|       58|       51|    52.75|7326300|    52.75|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
      "|   2|   TXG|2019-09-13|    52.75|   54.355|49.150002|    52.27|1025200|    52.27|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
      "|   3|   TXG|2019-09-16|52.450001|       56|52.009998|55.200001| 269900|55.200001|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
      "|   4|   TXG|2019-09-17|56.209999|60.900002|   55.423|56.779999| 602800|56.779999|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
      "|   5|   TXG|2019-09-18|56.849998|    62.27|55.650002|       62|1589600|       62|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
      "|   6|   TXG|2019-09-19|62.810001|   63.375|61.029999|61.119999| 425200|61.119999|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
      "|   7|   TXG|2019-09-20|61.709999|62.419998|    59.82|     60.5| 392000|     60.5|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
      "|   8|   TXG|2019-09-23|60.220001|61.485001|59.939999|60.330002| 137200|60.330002|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
      "|   9|   TXG|2019-09-24|       61|       61|       54|54.299999| 713800|54.299999|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
      "|  10|   TXG|2019-09-25|54.459999|55.880001|   52.563|52.759998| 261200|52.759998|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
      "|  11|   TXG|2019-09-26|52.779999|53.689999|46.619999|49.990002| 596300|49.990002|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
      "|  12|   TXG|2019-09-27|51.130001|       55|50.700001|51.029999| 621300|51.029999|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
      "|  13|   TXG|2019-09-30|51.049999|       52|    49.25|50.400002| 168900|50.400002|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
      "|  14|   TXG|2019-10-01|50.509998|51.919998|       46|47.029999| 536300|47.029999|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
      "|  15|   TXG|2019-10-02|46.779999|    47.23|45.110001|    46.07| 519600|    46.07|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
      "|  16|   TXG|2019-10-03|    46.77|48.240002|    45.75|48.119999| 703900|48.119999|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
      "|  17|   TXG|2019-10-04|       48|    53.34|    47.82|51.450001| 322400|51.450001|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
      "|  18|   TXG|2019-10-07|52.099998|53.220001|49.029999|50.360001| 476600|50.360001|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
      "|  19|   TXG|2019-10-08|       50|    51.27|       49|49.549999| 284100|49.549999|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
      "+----+------+----------+---------+---------+---------+---------+-------+---------+----------+-------------+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7774490-ff2d-4d73-b4cf-e72566f48d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before structuring schema\n",
    "\n",
    "data = spark.read.csv(\n",
    "    'data/stocks_price_final.csv',\n",
    "    sep = ',',\n",
    "    header = True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28df03e7-304e-458e-8c64-1ccf935d4028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- symbol: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- open: string (nullable = true)\n",
      " |-- high: string (nullable = true)\n",
      " |-- low: string (nullable = true)\n",
      " |-- close: string (nullable = true)\n",
      " |-- volume: string (nullable = true)\n",
      " |-- adjusted: string (nullable = true)\n",
      " |-- market.cap: string (nullable = true)\n",
      " |-- sector: string (nullable = true)\n",
      " |-- industry: string (nullable = true)\n",
      " |-- exchange: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8ce6959-6dd8-4f57-879f-646bf2e3e9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5ea657e-77de-47e7-a054-35621e9af1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_schema = [\n",
    "               StructField('_c0', IntegerType(), True),\n",
    "               StructField('symbol', StringType(), True),\n",
    "               StructField('data', DateType(), True),\n",
    "               StructField('open', DoubleType(), True),\n",
    "               StructField('high', DoubleType(), True),\n",
    "               StructField('low', DoubleType(), True),\n",
    "               StructField('close', DoubleType(), True),\n",
    "               StructField('volume', IntegerType(), True),\n",
    "               StructField('adjusted', DoubleType(), True),\n",
    "               StructField('market.cap', StringType(), True),\n",
    "               StructField('sector', StringType(), True),\n",
    "               StructField('industry', StringType(), True),\n",
    "               StructField('exchange', StringType(), True),\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab853baa-5740-4fea-8c6e-4726046ce298",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_struc = StructType(fields = data_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7ac8420-7841-49e7-b930-d8775e34072d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.csv(\n",
    "    'data/stocks_price_final.csv',\n",
    "    sep = ',',\n",
    "    header = True,\n",
    "    schema = final_struc \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "edb4bfb5-ef74-4a81-9106-9f1c23c01d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- symbol: string (nullable = true)\n",
      " |-- data: date (nullable = true)\n",
      " |-- open: double (nullable = true)\n",
      " |-- high: double (nullable = true)\n",
      " |-- low: double (nullable = true)\n",
      " |-- close: double (nullable = true)\n",
      " |-- volume: integer (nullable = true)\n",
      " |-- adjusted: double (nullable = true)\n",
      " |-- market.cap: string (nullable = true)\n",
      " |-- sector: string (nullable = true)\n",
      " |-- industry: string (nullable = true)\n",
      " |-- exchange: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "12d73cba-42df-4717-a19e-ed902fc1236e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(_c0,IntegerType,true),StructField(symbol,StringType,true),StructField(data,DateType,true),StructField(open,DoubleType,true),StructField(high,DoubleType,true),StructField(low,DoubleType,true),StructField(close,DoubleType,true),StructField(volume,IntegerType,true),StructField(adjusted,DoubleType,true),StructField(market.cap,StringType,true),StructField(sector,StringType,true),StructField(industry,StringType,true),StructField(exchange,StringType,true)))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "395f5dd1-209c-472e-90aa-c24ee75d6ea8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_c0', 'int'),\n",
       " ('symbol', 'string'),\n",
       " ('data', 'date'),\n",
       " ('open', 'double'),\n",
       " ('high', 'double'),\n",
       " ('low', 'double'),\n",
       " ('close', 'double'),\n",
       " ('volume', 'int'),\n",
       " ('adjusted', 'double'),\n",
       " ('market.cap', 'string'),\n",
       " ('sector', 'string'),\n",
       " ('industry', 'string'),\n",
       " ('exchange', 'string')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4121b3d2-6394-4138-900d-a4ba54d5b534",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_c0=1, symbol='TXG', data=datetime.date(2019, 9, 12), open=54.0, high=58.0, low=51.0, close=52.75, volume=7326300, adjusted=52.75, market.cap='$9.31B', sector='Capital Goods', industry='Biotechnology: Laboratory Analytical Instruments', exchange='NASDAQ'),\n",
       " Row(_c0=2, symbol='TXG', data=datetime.date(2019, 9, 13), open=52.75, high=54.355, low=49.150002, close=52.27, volume=1025200, adjusted=52.27, market.cap='$9.31B', sector='Capital Goods', industry='Biotechnology: Laboratory Analytical Instruments', exchange='NASDAQ'),\n",
       " Row(_c0=3, symbol='TXG', data=datetime.date(2019, 9, 16), open=52.450001, high=56.0, low=52.009998, close=55.200001, volume=269900, adjusted=55.200001, market.cap='$9.31B', sector='Capital Goods', industry='Biotechnology: Laboratory Analytical Instruments', exchange='NASDAQ'),\n",
       " Row(_c0=4, symbol='TXG', data=datetime.date(2019, 9, 17), open=56.209999, high=60.900002, low=55.423, close=56.779999, volume=602800, adjusted=56.779999, market.cap='$9.31B', sector='Capital Goods', industry='Biotechnology: Laboratory Analytical Instruments', exchange='NASDAQ'),\n",
       " Row(_c0=5, symbol='TXG', data=datetime.date(2019, 9, 18), open=56.849998, high=62.27, low=55.650002, close=62.0, volume=1589600, adjusted=62.0, market.cap='$9.31B', sector='Capital Goods', industry='Biotechnology: Laboratory Analytical Instruments', exchange='NASDAQ'),\n",
       " Row(_c0=6, symbol='TXG', data=datetime.date(2019, 9, 19), open=62.810001, high=63.375, low=61.029999, close=61.119999, volume=425200, adjusted=61.119999, market.cap='$9.31B', sector='Capital Goods', industry='Biotechnology: Laboratory Analytical Instruments', exchange='NASDAQ'),\n",
       " Row(_c0=7, symbol='TXG', data=datetime.date(2019, 9, 20), open=61.709999, high=62.419998, low=59.82, close=60.5, volume=392000, adjusted=60.5, market.cap='$9.31B', sector='Capital Goods', industry='Biotechnology: Laboratory Analytical Instruments', exchange='NASDAQ'),\n",
       " Row(_c0=8, symbol='TXG', data=datetime.date(2019, 9, 23), open=60.220001, high=61.485001, low=59.939999, close=60.330002, volume=137200, adjusted=60.330002, market.cap='$9.31B', sector='Capital Goods', industry='Biotechnology: Laboratory Analytical Instruments', exchange='NASDAQ'),\n",
       " Row(_c0=9, symbol='TXG', data=datetime.date(2019, 9, 24), open=61.0, high=61.0, low=54.0, close=54.299999, volume=713800, adjusted=54.299999, market.cap='$9.31B', sector='Capital Goods', industry='Biotechnology: Laboratory Analytical Instruments', exchange='NASDAQ'),\n",
       " Row(_c0=10, symbol='TXG', data=datetime.date(2019, 9, 25), open=54.459999, high=55.880001, low=52.563, close=52.759998, volume=261200, adjusted=52.759998, market.cap='$9.31B', sector='Capital Goods', industry='Biotechnology: Laboratory Analytical Instruments', exchange='NASDAQ')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9aa1ad45-dd89-4655-a008-670549a9fb8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "95967a39-5c34-43a3-af24-a03153ed01e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+---------+---------+---------+---------+-------+---------+----------+-------------+--------------------+--------+\n",
      "|_c0|symbol|      data|     open|     high|      low|    close| volume| adjusted|market.cap|       sector|            industry|exchange|\n",
      "+---+------+----------+---------+---------+---------+---------+-------+---------+----------+-------------+--------------------+--------+\n",
      "|  1|   TXG|2019-09-12|     54.0|     58.0|     51.0|    52.75|7326300|    52.75|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
      "|  2|   TXG|2019-09-13|    52.75|   54.355|49.150002|    52.27|1025200|    52.27|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
      "|  3|   TXG|2019-09-16|52.450001|     56.0|52.009998|55.200001| 269900|55.200001|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
      "|  4|   TXG|2019-09-17|56.209999|60.900002|   55.423|56.779999| 602800|56.779999|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
      "|  5|   TXG|2019-09-18|56.849998|    62.27|55.650002|     62.0|1589600|     62.0|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
      "|  6|   TXG|2019-09-19|62.810001|   63.375|61.029999|61.119999| 425200|61.119999|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
      "|  7|   TXG|2019-09-20|61.709999|62.419998|    59.82|     60.5| 392000|     60.5|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
      "|  8|   TXG|2019-09-23|60.220001|61.485001|59.939999|60.330002| 137200|60.330002|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
      "|  9|   TXG|2019-09-24|     61.0|     61.0|     54.0|54.299999| 713800|54.299999|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
      "| 10|   TXG|2019-09-25|54.459999|55.880001|   52.563|52.759998| 261200|52.759998|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
      "| 11|   TXG|2019-09-26|52.779999|53.689999|46.619999|49.990002| 596300|49.990002|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
      "| 12|   TXG|2019-09-27|51.130001|     55.0|50.700001|51.029999| 621300|51.029999|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
      "| 13|   TXG|2019-09-30|51.049999|     52.0|    49.25|50.400002| 168900|50.400002|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
      "| 14|   TXG|2019-10-01|50.509998|51.919998|     46.0|47.029999| 536300|47.029999|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
      "| 15|   TXG|2019-10-02|46.779999|    47.23|45.110001|    46.07| 519600|    46.07|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
      "| 16|   TXG|2019-10-03|    46.77|48.240002|    45.75|48.119999| 703900|48.119999|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
      "| 17|   TXG|2019-10-04|     48.0|    53.34|    47.82|51.450001| 322400|51.450001|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
      "| 18|   TXG|2019-10-07|52.099998|53.220001|49.029999|50.360001| 476600|50.360001|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
      "| 19|   TXG|2019-10-08|     50.0|    51.27|     49.0|49.549999| 284100|49.549999|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
      "| 20|   TXG|2019-10-09|49.630001|51.525002|49.575001|50.009998| 201100|50.009998|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
      "+---+------+----------+---------+---------+---------+---------+-------+---------+----------+-------------+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aa5084d6-91c9-43fc-85cc-a47f1b66c798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method show in module pyspark.sql.dataframe:\n",
      "\n",
      "show(n=20, truncate=True, vertical=False) method of pyspark.sql.dataframe.DataFrame instance\n",
      "    Prints the first ``n`` rows to the console.\n",
      "    \n",
      "    .. versionadded:: 1.3.0\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    n : int, optional\n",
      "        Number of rows to show.\n",
      "    truncate : bool or int, optional\n",
      "        If set to ``True``, truncate strings longer than 20 chars by default.\n",
      "        If set to a number greater than one, truncates long strings to length ``truncate``\n",
      "        and align cells right.\n",
      "    vertical : bool, optional\n",
      "        If set to ``True``, print output rows vertically (one line\n",
      "        per column value).\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df\n",
      "    DataFrame[age: int, name: string]\n",
      "    >>> df.show()\n",
      "    +---+-----+\n",
      "    |age| name|\n",
      "    +---+-----+\n",
      "    |  2|Alice|\n",
      "    |  5|  Bob|\n",
      "    +---+-----+\n",
      "    >>> df.show(truncate=3)\n",
      "    +---+----+\n",
      "    |age|name|\n",
      "    +---+----+\n",
      "    |  2| Ali|\n",
      "    |  5| Bob|\n",
      "    +---+----+\n",
      "    >>> df.show(vertical=True)\n",
      "    -RECORD 0-----\n",
      "     age  | 2\n",
      "     name | Alice\n",
      "    -RECORD 1-----\n",
      "     age  | 5\n",
      "     name | Bob\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(data.show)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3fe3a4-b28b-43a8-99c8-2e0e86e0885c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e490e737-2e94-4067-ad75-a4067afad42f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3419a987-2572-4c3b-8c8b-146500c2979e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
